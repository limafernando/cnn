{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carrega o banco de dados do mnist\n",
    "\n",
    "(xTrain, yTrain), (xTest, yTest) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xTrain\n",
    "len(xTrain)\n",
    "#xTrain.shape\n",
    "#yTrain\n",
    "#yTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One-hot encoding\n",
    "\n",
    "yTreino = to_categorical(yTrain)\n",
    "yTreino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F1C55D610B8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualizando exemplo de imagem\n",
    "\n",
    "data = xTrain[0]\n",
    "img = Image.fromarray(data, 'L') #'L' para imagem em escala de cinza (8-bit)\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABz0lEQVR4nLWRP2iTURTFf/e9L1+0aRQRUaS2koZYQ5cMLmoxTp11ERwcpFtdhCIODoJrXeokIiI6iIsIim4VFHSRgtqAtXRItdHaEkwoSZN87zq0NX/6pZt3e/eee84958F/L9n2ViuigWdwDW0DCBgAaezIAOyN964tHu+zK3MlUbxWcK+/u08r55OJ2Qd3h3k1kbONJsAkov1nBg5lAarcSP1eeLqI25IQ5djjdOBbsVDzp+/sCQp/vpZF2WRQWI70gErwfXnIn3mxq9LYaDclilOXlvpPaWGscGT8V70ekkg8m7zwo3gVSA+JNbLpr+mi/IboSmLN88hBEMJg/Ejynn7aD9YLGW+wJd/rlYPxkPA2gzKMlFZfTmbEdEEY/LGSqz0/2dONRPBHp1XfnbNdZSAzMa+fx0O/ETA+MKWaS20h2hyJCWqS3nc4CKot1lrWXWASp0fTCWvn5xBtB4jiOJC9PBKDSv6LcSEHxM4+LKqr1/K3BztPFGOID0wWaw3VhZuDkRAH9tbr2bJTzV0bjtHhUbCZ649+OlV190/EoDNpj9QzVdWZD08uHgVjOqY4qm+X1s3qx/VveTwXdv6/itptLQEQTwDHzttd6i+Gp6IkY/vpNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x7F1C55D7F3C8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, h = 32,32\n",
    "img = img.resize((w,h), Image.ANTIALIAS)\n",
    "img\n",
    "\n",
    "a = np.asarray(img)\n",
    "\n",
    "img = Image.fromarray(a, 'L') #'L' para imagem em escala de cinza (8-bit)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 32, 32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ajuste das dimensões das imagens de treino\n",
    "\n",
    "w, h = 32, 32\n",
    "\n",
    "auxList = []\n",
    "\n",
    "for i in range(len(xTrain)):\n",
    "    \n",
    "    img = Image.fromarray(xTrain[i], 'L') #'L' para imagem em escala de cinza (8-bit)\n",
    "\n",
    "    img = img.resize((w,h), Image.ANTIALIAS)\n",
    "    \n",
    "    auxList.append(np.asarray(img))\n",
    "\n",
    "xTreino = np.array(auxList)        \n",
    "xTreino.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ajuste das dimensões das imagens de teste\n",
    "\n",
    "w, h = 32, 32\n",
    "\n",
    "auxList = []\n",
    "\n",
    "for i in range(len(xTest)):\n",
    "    \n",
    "    img = Image.fromarray(xTest[i], 'L') #'L' para imagem em escala de cinza (8-bit)\n",
    "\n",
    "    img = img.resize((w,h), Image.ANTIALIAS)\n",
    "\n",
    "    auxList.append(np.asarray(img))\n",
    "\n",
    "xTeste = np.array(auxList)\n",
    "xTeste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABz0lEQVR4nLWRP2iTURTFf/e9L1+0aRQRUaS2koZYQ5cMLmoxTp11ERwcpFtdhCIODoJrXeokIiI6iIsIim4VFHSRgtqAtXRItdHaEkwoSZN87zq0NX/6pZt3e/eee84958F/L9n2ViuigWdwDW0DCBgAaezIAOyN964tHu+zK3MlUbxWcK+/u08r55OJ2Qd3h3k1kbONJsAkov1nBg5lAarcSP1eeLqI25IQ5djjdOBbsVDzp+/sCQp/vpZF2WRQWI70gErwfXnIn3mxq9LYaDclilOXlvpPaWGscGT8V70ekkg8m7zwo3gVSA+JNbLpr+mi/IboSmLN88hBEMJg/Ejynn7aD9YLGW+wJd/rlYPxkPA2gzKMlFZfTmbEdEEY/LGSqz0/2dONRPBHp1XfnbNdZSAzMa+fx0O/ETA+MKWaS20h2hyJCWqS3nc4CKot1lrWXWASp0fTCWvn5xBtB4jiOJC9PBKDSv6LcSEHxM4+LKqr1/K3BztPFGOID0wWaw3VhZuDkRAH9tbr2bJTzV0bjtHhUbCZ649+OlV190/EoDNpj9QzVdWZD08uHgVjOqY4qm+X1s3qx/VveTwXdv6/itptLQEQTwDHzttd6i+Gp6IkY/vpNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x7F1C55D5D3C8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualizando exemplo de imagem\n",
    "\n",
    "data = xTreino[0]\n",
    "img = Image.fromarray(data, 'L') #'L' para imagem em escala de cinza (8-bit)\n",
    "\n",
    "print(data.shape)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leNet():\n",
    "    \n",
    "    classificador = Sequential()\n",
    "    \n",
    "    #imagem 32x32\n",
    "    #Camada convolucional 1\n",
    "    classificador.add(Conv2D(filters = 6, kernel_size = 5, strides = 1, activation = 'relu', input_shape = (32,32,1)))\n",
    "    \n",
    "    #imagem 28x28\n",
    "    #Camada de pooling 1\n",
    "    classificador.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    \n",
    "    #imagem 14x14\n",
    "    #Camada convolucional 2\n",
    "    classificador.add(Conv2D(filters = 16, kernel_size = 5, strides = 1, activation = 'relu', input_shape = (14,14,6)))\n",
    "    \n",
    "    #imagem 10x10\n",
    "    #Camada de pooling 2\n",
    "    classificador.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    \n",
    "    #imagem 5x5\n",
    "    #Flatten\n",
    "    classificador.add(Flatten())\n",
    "    \n",
    "    #Camada totalmente conectada 1\n",
    "    classificador.add(Dense(units = 120, activation = 'relu'))\n",
    "    \n",
    "    #Camada totalmente conectada 2\n",
    "    classificador.add(Dense(units = 84, activation = 'relu'))\n",
    "    \n",
    "    #Camada de saída\n",
    "    classificador.add(Dense(units = 10, activation = 'softmax'))\n",
    "    \n",
    "    classificador.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return  classificador"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
